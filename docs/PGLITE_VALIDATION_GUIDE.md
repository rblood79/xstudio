# PGlite ê²€ì¦ ë° í”„ë¡œë•ì…˜ ì¤€ë¹„ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-11-07
**ëª©ì **: PGlite ë°°í¬ ì „ ì„±ëŠ¥/ì•ˆì •ì„±/ë°±ì—…/ë™ê¸°í™” ì „ëµ ê²€ì¦

---

## ğŸ“‹ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ë°°í¬ ì „ ë°˜ë“œì‹œ ì™„ë£Œí•´ì•¼ í•  ê²€ì¦ í•­ëª©:

- [ ] **1. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹** (ì˜ˆìƒ ì‹œê°„: 1-2ì¼)
  - [ ] CRUD ì„±ëŠ¥ ì¸¡ì •
  - [ ] RPC í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì •
  - [ ] ë™ì‹œì„± í…ŒìŠ¤íŠ¸
  - [ ] ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

- [ ] **2. ë°ì´í„° ê´€ë¦¬ ì „ëµ** (ì˜ˆìƒ ì‹œê°„: 1ì¼)
  - [ ] ì €ì¥ ê²½ë¡œ ì •ì±…
  - [ ] ë°±ì—… ì „ëµ
  - [ ] ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
  - [ ] ë””ìŠ¤í¬ ê³µê°„ ê´€ë¦¬

- [ ] **3. ë™ê¸°í™” í”„ë¡œí† íƒ€ì…** (ì˜ˆìƒ ì‹œê°„: 2-3ì¼)
  - [ ] ì¦ë¶„ ë™ê¸°í™” ì„¤ê³„
  - [ ] ì¶©ëŒ í•´ê²° ì •ì±…
  - [ ] ì˜¤í”„ë¼ì¸ íì‰
  - [ ] ì¬ì—°ê²° ë¡œì§

- [ ] **4. ì•ˆì •ì„± í…ŒìŠ¤íŠ¸** (ì˜ˆìƒ ì‹œê°„: 1-2ì¼)
  - [ ] í¬ë˜ì‹œ ë³µêµ¬
  - [ ] ë°ì´í„° ë¬´ê²°ì„±
  - [ ] íŠ¸ëœì­ì…˜ ë¡¤ë°±
  - [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜

---

## 1ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

### 1.1. CRUD ì„±ëŠ¥ ì¸¡ì •

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```typescript
// tests/benchmarks/crud-performance.test.ts
import { performance } from 'perf_hooks';
import { db } from '../../src/services/database';

describe('PGlite CRUD Performance Benchmarks', () => {
  let testProjectId: string;

  beforeAll(async () => {
    await db.initialize();

    // í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸ ìƒì„±
    const [project] = await db.insert('projects', {
      name: 'Performance Test Project',
    });
    testProjectId = project.id;
  });

  afterAll(async () => {
    // í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
    await db.delete('projects', testProjectId);
    await db.close();
  });

  describe('INSERT Performance', () => {
    it('should insert 1,000 elements in < 2 seconds', async () => {
      const elements = Array.from({ length: 1000 }, (_, i) => ({
        page_id: testProjectId,
        tag: 'Button',
        props: { variant: 'primary', label: `Button ${i}` },
        order_num: i,
      }));

      const start = performance.now();
      await db.insert('elements', elements);
      const duration = performance.now() - start;

      console.log(`âœ… Inserted 1,000 elements in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(2000); // < 2ì´ˆ
    });

    it('should batch insert 10,000 elements in < 10 seconds', async () => {
      const batchSize = 1000;
      const totalElements = 10000;
      const start = performance.now();

      for (let i = 0; i < totalElements; i += batchSize) {
        const batch = Array.from({ length: batchSize }, (_, j) => ({
          page_id: testProjectId,
          tag: 'Text',
          props: { content: `Text ${i + j}` },
          order_num: i + j,
        }));

        await db.insert('elements', batch);
      }

      const duration = performance.now() - start;

      console.log(`âœ… Batch inserted 10,000 elements in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(10000); // < 10ì´ˆ
    });
  });

  describe('SELECT Performance', () => {
    beforeAll(async () => {
      // 10,000ê°œ ìš”ì†Œ ì‚½ì…
      const elements = Array.from({ length: 10000 }, (_, i) => ({
        page_id: testProjectId,
        tag: 'Button',
        props: { variant: i % 3 === 0 ? 'primary' : 'secondary', label: `Button ${i}` },
        order_num: i,
      }));
      await db.insert('elements', elements);
    });

    it('should select 10,000 elements in < 500ms', async () => {
      const start = performance.now();
      const elements = await db.select('elements', {
        where: { page_id: testProjectId },
        orderBy: [{ column: 'order_num', ascending: true }],
      });
      const duration = performance.now() - start;

      console.log(`âœ… Selected ${elements.length} elements in ${duration.toFixed(2)}ms`);
      expect(elements.length).toBe(10000);
      expect(duration).toBeLessThan(500); // < 500ms
    });

    it('should filter by JSONB property in < 200ms', async () => {
      const start = performance.now();
      const primaryButtons = await db.query(
        "SELECT * FROM elements WHERE props->>'variant' = $1",
        ['primary']
      );
      const duration = performance.now() - start;

      console.log(`âœ… Filtered ${primaryButtons.length} elements by JSONB in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(200); // < 200ms
    });
  });

  describe('UPDATE Performance', () => {
    it('should update 1,000 elements in < 1 second', async () => {
      const elements = await db.select('elements', {
        where: { page_id: testProjectId },
        limit: 1000,
      });

      const start = performance.now();
      for (const element of elements) {
        await db.update('elements', element.id, {
          props: { ...element.props, updated: true },
        });
      }
      const duration = performance.now() - start;

      console.log(`âœ… Updated 1,000 elements in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(1000); // < 1ì´ˆ
    });

    it('should batch update with transaction in < 500ms', async () => {
      const elements = await db.select('elements', {
        where: { page_id: testProjectId },
        limit: 1000,
      });

      const start = performance.now();
      await db.transaction(async (tx) => {
        for (const element of elements) {
          await tx.query(
            'UPDATE elements SET props = $1 WHERE id = $2',
            [JSON.stringify({ ...element.props, batch_updated: true }), element.id]
          );
        }
      });
      const duration = performance.now() - start;

      console.log(`âœ… Batch updated 1,000 elements in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(500); // < 500ms
    });
  });

  describe('DELETE Performance', () => {
    it('should delete 1,000 elements in < 500ms', async () => {
      const elements = await db.select('elements', {
        where: { page_id: testProjectId },
        limit: 1000,
      });

      const start = performance.now();
      for (const element of elements) {
        await db.delete('elements', element.id);
      }
      const duration = performance.now() - start;

      console.log(`âœ… Deleted 1,000 elements in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(500); // < 500ms
    });

    it('should cascade delete page with 10,000 elements in < 2 seconds', async () => {
      // ìƒˆ í˜ì´ì§€ ìƒì„±
      const [page] = await db.insert('pages', {
        project_id: testProjectId,
        title: 'Cascade Test Page',
        slug: 'cascade-test',
      });

      // 10,000ê°œ ìš”ì†Œ ì‚½ì…
      const elements = Array.from({ length: 10000 }, (_, i) => ({
        page_id: page.id,
        tag: 'Text',
        props: { content: `Text ${i}` },
        order_num: i,
      }));
      await db.insert('elements', elements);

      // í˜ì´ì§€ ì‚­ì œ (CASCADEë¡œ ëª¨ë“  ìš”ì†Œë„ ì‚­ì œë¨)
      const start = performance.now();
      await db.delete('pages', page.id);
      const duration = performance.now() - start;

      console.log(`âœ… Cascade deleted page with 10,000 elements in ${duration.toFixed(2)}ms`);
      expect(duration).toBeLessThan(2000); // < 2ì´ˆ
    });
  });
});
```

#### ì„±ëŠ¥ ê¸°ì¤€ (ê¶Œì¥)

| ì‘ì—… | ë°ì´í„° í¬ê¸° | ëª©í‘œ ì‹œê°„ | í—ˆìš© ì‹œê°„ |
|------|------------|----------|----------|
| INSERT (ë‹¨ì¼) | 1ê°œ | < 5ms | < 20ms |
| INSERT (ë°°ì¹˜) | 1,000ê°œ | < 500ms | < 2ì´ˆ |
| SELECT (ì „ì²´) | 10,000ê°œ | < 200ms | < 500ms |
| SELECT (JSONB í•„í„°) | 10,000ê°œ | < 100ms | < 200ms |
| UPDATE (ë‹¨ì¼) | 1ê°œ | < 5ms | < 20ms |
| UPDATE (íŠ¸ëœì­ì…˜) | 1,000ê°œ | < 200ms | < 500ms |
| DELETE (ë‹¨ì¼) | 1ê°œ | < 5ms | < 20ms |
| DELETE (CASCADE) | 10,000ê°œ | < 1ì´ˆ | < 2ì´ˆ |

---

### 1.2. RPC í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì •

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```typescript
// tests/benchmarks/rpc-performance.test.ts
import { performance } from 'perf_hooks';
import { db } from '../../src/services/database';

describe('PGlite RPC Performance Benchmarks', () => {
  let testThemeId: string;
  let parentThemeId: string;

  beforeAll(async () => {
    await db.initialize();

    // í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸ ë° í…Œë§ˆ ìƒì„±
    const [project] = await db.insert('projects', {
      name: 'RPC Test Project',
    });

    // ë¶€ëª¨ í…Œë§ˆ ìƒì„±
    const [parentTheme] = await db.insert('design_themes', {
      project_id: project.id,
      name: 'Parent Theme',
      status: 'active',
    });
    parentThemeId = parentTheme.id;

    // ìì‹ í…Œë§ˆ ìƒì„± (ìƒì†)
    const [childTheme] = await db.insert('design_themes', {
      project_id: project.id,
      name: 'Child Theme',
      parent_theme_id: parentThemeId,
      status: 'active',
    });
    testThemeId = childTheme.id;

    // ë¶€ëª¨ í…Œë§ˆì— í† í° 1,000ê°œ ì‚½ì…
    const tokens = Array.from({ length: 1000 }, (_, i) => ({
      project_id: project.id,
      theme_id: parentThemeId,
      name: `color.shade.${i}`,
      type: 'color',
      value: { h: i % 360, s: 50, l: 50, a: 1 },
      scope: 'raw',
    }));
    await db.insert('design_tokens', tokens);

    // ìì‹ í…Œë§ˆì— í† í° 100ê°œ ì‚½ì… (ì˜¤ë²„ë¼ì´ë“œ)
    const childTokens = Array.from({ length: 100 }, (_, i) => ({
      project_id: project.id,
      theme_id: testThemeId,
      name: `color.shade.${i}`,
      type: 'color',
      value: { h: i % 360, s: 70, l: 60, a: 1 },
      scope: 'raw',
    }));
    await db.insert('design_tokens', childTokens);
  });

  describe('resolve_theme_tokens', () => {
    it('should resolve 1,100 tokens (with inheritance) in < 200ms', async () => {
      const start = performance.now();
      const tokens = await db.rpc('resolve_theme_tokens', {
        p_theme_id: testThemeId,
      });
      const duration = performance.now() - start;

      console.log(`âœ… Resolved ${tokens.length} tokens in ${duration.toFixed(2)}ms`);
      expect(tokens.length).toBe(1100); // 100 (child) + 1000 (parent)
      expect(duration).toBeLessThan(200); // < 200ms
    });

    it('should handle 5-level deep inheritance in < 500ms', async () => {
      // 5ë‹¨ê³„ ìƒì† êµ¬ì¡° ìƒì„±
      let currentThemeId = parentThemeId;
      const [project] = await db.select('projects', { limit: 1 });

      for (let i = 1; i <= 5; i++) {
        const [theme] = await db.insert('design_themes', {
          project_id: project.id,
          name: `Level ${i} Theme`,
          parent_theme_id: currentThemeId,
          status: 'active',
        });

        // ê° ë ˆë²¨ì— í† í° 50ê°œ ì¶”ê°€
        const tokens = Array.from({ length: 50 }, (_, j) => ({
          project_id: project.id,
          theme_id: theme.id,
          name: `level${i}.token.${j}`,
          type: 'color',
          value: { h: j * 7, s: 50, l: 50, a: 1 },
          scope: 'raw',
        }));
        await db.insert('design_tokens', tokens);

        currentThemeId = theme.id;
      }

      // ê°€ì¥ ê¹Šì€ ë ˆë²¨ í…Œë§ˆì˜ í† í° í•´ì„
      const start = performance.now();
      const tokens = await db.rpc('resolve_theme_tokens', {
        p_theme_id: currentThemeId,
      });
      const duration = performance.now() - start;

      console.log(`âœ… Resolved ${tokens.length} tokens (5-level) in ${duration.toFixed(2)}ms`);
      expect(tokens.length).toBeGreaterThan(0);
      expect(duration).toBeLessThan(500); // < 500ms
    });
  });

  describe('duplicate_theme', () => {
    it('should duplicate theme with 1,000 tokens in < 1 second', async () => {
      const start = performance.now();
      const newThemeId = await db.rpc('duplicate_theme', {
        p_source_theme_id: parentThemeId,
        p_new_name: 'Duplicated Theme',
        p_inherit: false, // í† í° ë³µì‚¬
      });
      const duration = performance.now() - start;

      console.log(`âœ… Duplicated theme with 1,000 tokens in ${duration.toFixed(2)}ms`);
      expect(newThemeId).toBeTruthy();
      expect(duration).toBeLessThan(1000); // < 1ì´ˆ

      // í† í° ë³µì‚¬ í™•ì¸
      const tokens = await db.select('design_tokens', {
        where: { theme_id: newThemeId },
      });
      expect(tokens.length).toBe(1000);
    });

    it('should create inherited theme (no token copy) in < 50ms', async () => {
      const start = performance.now();
      const newThemeId = await db.rpc('duplicate_theme', {
        p_source_theme_id: parentThemeId,
        p_new_name: 'Inherited Theme',
        p_inherit: true, // í† í° ë³µì‚¬ ì•ˆ í•¨
      });
      const duration = performance.now() - start;

      console.log(`âœ… Created inherited theme in ${duration.toFixed(2)}ms`);
      expect(newThemeId).toBeTruthy();
      expect(duration).toBeLessThan(50); // < 50ms

      // í† í° ë³µì‚¬ ì•ˆ ë¨ í™•ì¸
      const tokens = await db.select('design_tokens', {
        where: { theme_id: newThemeId },
      });
      expect(tokens.length).toBe(0);
    });
  });

  describe('search_tokens', () => {
    it('should search 1,000 tokens in < 100ms', async () => {
      const start = performance.now();
      const results = await db.rpc('search_tokens', {
        p_theme_id: testThemeId,
        p_query: 'color',
        p_include_inherited: true,
      });
      const duration = performance.now() - start;

      console.log(`âœ… Searched ${results.length} tokens in ${duration.toFixed(2)}ms`);
      expect(results.length).toBeGreaterThan(0);
      expect(duration).toBeLessThan(100); // < 100ms
    });
  });

  describe('bulk_upsert_tokens', () => {
    it('should upsert 500 tokens in < 500ms', async () => {
      const [project] = await db.select('projects', { limit: 1 });
      const [theme] = await db.select('design_themes', { limit: 1 });

      const tokens = Array.from({ length: 500 }, (_, i) => ({
        project_id: project.id,
        theme_id: theme.id,
        name: `bulk.token.${i}`,
        type: 'spacing',
        value: { value: i * 4, unit: 'px' },
        scope: 'raw',
      }));

      const start = performance.now();
      const count = await db.rpc('bulk_upsert_tokens', {
        p_tokens: tokens,
      });
      const duration = performance.now() - start;

      console.log(`âœ… Bulk upserted ${count} tokens in ${duration.toFixed(2)}ms`);
      expect(count).toBe(500);
      expect(duration).toBeLessThan(500); // < 500ms
    });
  });
});
```

#### RPC ì„±ëŠ¥ ê¸°ì¤€ (ê¶Œì¥)

| RPC í•¨ìˆ˜ | ë°ì´í„° í¬ê¸° | ëª©í‘œ ì‹œê°„ | í—ˆìš© ì‹œê°„ |
|----------|------------|----------|----------|
| `resolve_theme_tokens` | 1,000 í† í° | < 100ms | < 200ms |
| `resolve_theme_tokens` (5-level) | 250 í† í° | < 200ms | < 500ms |
| `duplicate_theme` (ë³µì‚¬) | 1,000 í† í° | < 500ms | < 1ì´ˆ |
| `duplicate_theme` (ìƒì†) | 0 í† í° | < 20ms | < 50ms |
| `search_tokens` | 1,000 í† í° | < 50ms | < 100ms |
| `bulk_upsert_tokens` | 500 í† í° | < 200ms | < 500ms |

---

### 1.3. ë™ì‹œì„± í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```typescript
// tests/benchmarks/concurrency.test.ts
import { performance } from 'perf_hooks';
import { db } from '../../src/services/database';

describe('PGlite Concurrency Tests', () => {
  it('should handle 10 concurrent inserts without errors', async () => {
    const [project] = await db.insert('projects', { name: 'Concurrency Test' });

    const start = performance.now();

    // 10ê°œ ë™ì‹œ ì‚½ì…
    const promises = Array.from({ length: 10 }, (_, i) =>
      db.insert('elements', {
        page_id: project.id,
        tag: 'Button',
        props: { label: `Button ${i}` },
        order_num: i,
      })
    );

    const results = await Promise.all(promises);
    const duration = performance.now() - start;

    console.log(`âœ… 10 concurrent inserts completed in ${duration.toFixed(2)}ms`);
    expect(results.length).toBe(10);
    expect(duration).toBeLessThan(200); // < 200ms
  });

  it('should handle transaction isolation correctly', async () => {
    const [project] = await db.insert('projects', { name: 'Transaction Test' });
    const [element] = await db.insert('elements', {
      page_id: project.id,
      tag: 'Counter',
      props: { count: 0 },
      order_num: 0,
    });

    // 2ê°œ íŠ¸ëœì­ì…˜ ë™ì‹œ ì‹¤í–‰ (ì¹´ìš´í„° ì¦ê°€)
    const tx1 = db.transaction(async (tx) => {
      const [el] = await tx.query('SELECT * FROM elements WHERE id = $1', [element.id]);
      await new Promise(resolve => setTimeout(resolve, 100)); // ì˜ë„ì  ì§€ì—°
      await tx.query('UPDATE elements SET props = $1 WHERE id = $2', [
        JSON.stringify({ count: el.props.count + 1 }),
        element.id,
      ]);
    });

    const tx2 = db.transaction(async (tx) => {
      const [el] = await tx.query('SELECT * FROM elements WHERE id = $1', [element.id]);
      await new Promise(resolve => setTimeout(resolve, 100)); // ì˜ë„ì  ì§€ì—°
      await tx.query('UPDATE elements SET props = $1 WHERE id = $2', [
        JSON.stringify({ count: el.props.count + 1 }),
        element.id,
      ]);
    });

    await Promise.all([tx1, tx2]);

    // ìµœì¢… ì¹´ìš´íŠ¸ í™•ì¸
    const [finalElement] = await db.query('SELECT * FROM elements WHERE id = $1', [element.id]);

    console.log(`âœ… Final count: ${finalElement.props.count}`);

    // íŠ¸ëœì­ì…˜ ê²©ë¦¬ë¡œ ì¸í•´ ì¹´ìš´íŠ¸ê°€ 2ì—¬ì•¼ í•¨
    expect(finalElement.props.count).toBe(2);
  });
});
```

---

### 1.4. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```typescript
// tests/benchmarks/large-data.test.ts
import { performance } from 'perf_hooks';
import { db } from '../../src/services/database';

describe('PGlite Large Data Tests', () => {
  it('should handle 100,000 elements without memory issues', async () => {
    const [project] = await db.insert('projects', { name: 'Large Data Test' });
    const [page] = await db.insert('pages', {
      project_id: project.id,
      title: 'Large Page',
      slug: 'large-page',
    });

    const batchSize = 1000;
    const totalElements = 100000;

    const start = performance.now();

    for (let i = 0; i < totalElements; i += batchSize) {
      const batch = Array.from({ length: batchSize }, (_, j) => ({
        page_id: page.id,
        tag: 'Text',
        props: { content: `Text ${i + j}` },
        order_num: i + j,
      }));

      await db.insert('elements', batch);

      // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
      if (i % 10000 === 0) {
        const memUsage = process.memoryUsage();
        console.log(`   ${i} elements inserted - Memory: ${(memUsage.heapUsed / 1024 / 1024).toFixed(2)} MB`);
      }
    }

    const duration = performance.now() - start;

    console.log(`âœ… Inserted 100,000 elements in ${(duration / 1000).toFixed(2)}s`);
    expect(duration).toBeLessThan(60000); // < 60ì´ˆ

    // ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
    const dbSize = await db.getDbSize();
    console.log(`   Database size: ${(dbSize / 1024 / 1024).toFixed(2)} MB`);
  });

  it('should query 100,000 elements with pagination efficiently', async () => {
    const pageSize = 100;
    const totalPages = 1000; // 100,000 / 100 = 1,000 í˜ì´ì§€

    const start = performance.now();

    for (let page = 0; page < 10; page++) { // ì²˜ìŒ 10 í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸
      const results = await db.query(
        'SELECT * FROM elements ORDER BY order_num LIMIT $1 OFFSET $2',
        [pageSize, page * pageSize]
      );

      expect(results.length).toBe(pageSize);
    }

    const duration = performance.now() - start;

    console.log(`âœ… Paginated through 1,000 elements in ${duration.toFixed(2)}ms`);
    expect(duration).toBeLessThan(500); // < 500ms for 10 pages
  });
});
```

---

## 2ï¸âƒ£ ë°ì´í„° ê´€ë¦¬ ì „ëµ

### 2.1. ì €ì¥ ê²½ë¡œ ì •ì±…

#### ìš´ì˜ ì²´ì œë³„ ê¸°ë³¸ ê²½ë¡œ

```typescript
// src/services/database/paths.ts

/**
 * Get PGlite database path based on OS
 */
export function getDefaultDbPath(): string {
  const platform = process.platform;
  const appName = 'xstudio';

  switch (platform) {
    case 'darwin': // macOS
      return `${process.env.HOME}/Library/Application Support/${appName}/database`;

    case 'win32': // Windows
      return `${process.env.APPDATA}\\${appName}\\database`;

    case 'linux': // Linux
      return `${process.env.HOME}/.config/${appName}/database`;

    default:
      return `./${appName}.pglite`;
  }
}

/**
 * Get backup directory path
 */
export function getBackupPath(): string {
  const dbPath = getDefaultDbPath();
  const backupDir = `${dbPath}_backups`;

  // Create backup directory if not exists
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true });
  }

  return backupDir;
}
```

#### ì‚¬ìš©ì ì •ì˜ ê²½ë¡œ ì„¤ì •

```typescript
// electron/main.ts

import { app, dialog } from 'electron';

// Settings ë©”ë‰´ì— ì¶”ê°€
ipcMain.handle('db:change-path', async () => {
  const result = await dialog.showOpenDialog({
    properties: ['openDirectory', 'createDirectory'],
    title: 'Select Database Location',
  });

  if (!result.canceled && result.filePaths.length > 0) {
    const newPath = result.filePaths[0];

    // ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
    await backupDatabase();

    // ìƒˆ ê²½ë¡œë¡œ ì´ë™
    await moveDatabase(getCurrentDbPath(), newPath);

    // ì„¤ì • ì €ì¥
    app.setPath('userData', newPath);

    return { success: true, path: newPath };
  }

  return { success: false };
});
```

---

### 2.2. ë°±ì—… ì „ëµ

#### ìë™ ë°±ì—… ì‹œìŠ¤í…œ

```typescript
// src/services/backup/autoBackup.ts

import * as fs from 'fs';
import * as path from 'path';
import { db } from '../database';
import { getBackupPath } from '../database/paths';

export class AutoBackupService {
  private backupInterval: NodeJS.Timeout | null = null;

  /**
   * Start automatic backup
   *
   * @param intervalHours - Backup interval in hours (default: 24)
   * @param maxBackups - Maximum number of backups to keep (default: 7)
   */
  start(intervalHours: number = 24, maxBackups: number = 7) {
    // ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ì¤‘ì§€
    if (this.backupInterval) {
      this.stop();
    }

    // ì¦‰ì‹œ ë°±ì—… ì‹¤í–‰
    this.createBackup(maxBackups);

    // ì£¼ê¸°ì  ë°±ì—… ì‹œì‘
    this.backupInterval = setInterval(() => {
      this.createBackup(maxBackups);
    }, intervalHours * 60 * 60 * 1000);

    console.log(`âœ… Auto backup started (interval: ${intervalHours}h, max: ${maxBackups})`);
  }

  /**
   * Stop automatic backup
   */
  stop() {
    if (this.backupInterval) {
      clearInterval(this.backupInterval);
      this.backupInterval = null;
      console.log('âœ… Auto backup stopped');
    }
  }

  /**
   * Create manual backup
   */
  async createBackup(maxBackups: number = 7): Promise<string> {
    const backupDir = getBackupPath();
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const backupName = `xstudio_backup_${timestamp}.pglite`;
    const backupPath = path.join(backupDir, backupName);

    try {
      // 1. VACUUM ì‹¤í–‰ (ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”)
      await db.vacuum();

      // 2. ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë³µì‚¬
      const dbPath = await db.getDbPath();
      await this.copyDirectory(dbPath, backupPath);

      console.log(`âœ… Backup created: ${backupPath}`);

      // 3. ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
      await this.cleanOldBackups(backupDir, maxBackups);

      return backupPath;
    } catch (error) {
      console.error('âŒ Backup failed:', error);
      throw error;
    }
  }

  /**
   * Restore from backup
   */
  async restoreBackup(backupPath: string): Promise<void> {
    try {
      // 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
      await db.close();

      // 2. í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (ì•ˆì „ë§)
      const currentDbPath = await db.getDbPath();
      const safetyBackup = `${currentDbPath}_before_restore`;
      await this.copyDirectory(currentDbPath, safetyBackup);

      // 3. ë°±ì—…ì—ì„œ ë³µì›
      await this.copyDirectory(backupPath, currentDbPath);

      // 4. ë°ì´í„°ë² ì´ìŠ¤ ì¬ì—°ê²°
      await db.initialize();

      console.log(`âœ… Restored from backup: ${backupPath}`);
    } catch (error) {
      console.error('âŒ Restore failed:', error);
      throw error;
    }
  }

  /**
   * List all backups
   */
  async listBackups(): Promise<Array<{ name: string; path: string; size: number; date: Date }>> {
    const backupDir = getBackupPath();
    const files = fs.readdirSync(backupDir);

    const backups = files
      .filter(file => file.startsWith('xstudio_backup_') && file.endsWith('.pglite'))
      .map(file => {
        const filePath = path.join(backupDir, file);
        const stats = fs.statSync(filePath);

        return {
          name: file,
          path: filePath,
          size: stats.size,
          date: stats.mtime,
        };
      })
      .sort((a, b) => b.date.getTime() - a.date.getTime());

    return backups;
  }

  /**
   * Delete old backups
   */
  private async cleanOldBackups(backupDir: string, maxBackups: number): Promise<void> {
    const backups = await this.listBackups();

    if (backups.length > maxBackups) {
      const toDelete = backups.slice(maxBackups);

      for (const backup of toDelete) {
        fs.rmSync(backup.path, { recursive: true, force: true });
        console.log(`ğŸ—‘ï¸ Deleted old backup: ${backup.name}`);
      }
    }
  }

  /**
   * Copy directory recursively
   */
  private async copyDirectory(src: string, dest: string): Promise<void> {
    if (fs.statSync(src).isDirectory()) {
      fs.mkdirSync(dest, { recursive: true });
      const files = fs.readdirSync(src);

      for (const file of files) {
        const srcPath = path.join(src, file);
        const destPath = path.join(dest, file);
        await this.copyDirectory(srcPath, destPath);
      }
    } else {
      fs.copyFileSync(src, dest);
    }
  }
}

// Export singleton
export const autoBackup = new AutoBackupService();
```

#### Electron Main Process í†µí•©

```typescript
// electron/main.ts

import { autoBackup } from '../src/services/backup/autoBackup';

app.whenReady().then(async () => {
  // ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
  await initializeDatabase();

  // ìë™ ë°±ì—… ì‹œì‘ (24ì‹œê°„ë§ˆë‹¤, ìµœëŒ€ 7ê°œ ë³´ê´€)
  autoBackup.start(24, 7);

  createWindow();
});

// IPC í•¸ë“¤ëŸ¬ ì¶”ê°€
ipcMain.handle('backup:create', async () => {
  const backupPath = await autoBackup.createBackup();
  return { success: true, path: backupPath };
});

ipcMain.handle('backup:list', async () => {
  return await autoBackup.listBackups();
});

ipcMain.handle('backup:restore', async (_event, backupPath: string) => {
  await autoBackup.restoreBackup(backupPath);
  return { success: true };
});
```

---

### 2.3. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

#### Export/Import ê¸°ëŠ¥

```typescript
// src/services/database/exportImport.ts

import * as fs from 'fs';
import { db } from './index';

export class ExportImportService {
  /**
   * Export database to JSON
   */
  async exportToJson(filePath: string): Promise<void> {
    const data: any = {};

    // ëª¨ë“  í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
    const tables = ['projects', 'pages', 'elements', 'design_themes', 'design_tokens'];

    for (const table of tables) {
      data[table] = await db.select(table);
    }

    // JSON íŒŒì¼ë¡œ ì €ì¥
    fs.writeFileSync(filePath, JSON.stringify(data, null, 2));
    console.log(`âœ… Exported to ${filePath}`);
  }

  /**
   * Import database from JSON
   */
  async importFromJson(filePath: string): Promise<void> {
    const data = JSON.parse(fs.readFileSync(filePath, 'utf-8'));

    await db.transaction(async (tx) => {
      // ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì—­ìˆœìœ¼ë¡œ)
      await tx.query('DELETE FROM design_tokens');
      await tx.query('DELETE FROM design_themes');
      await tx.query('DELETE FROM elements');
      await tx.query('DELETE FROM pages');
      await tx.query('DELETE FROM projects');

      // ìƒˆ ë°ì´í„° ì‚½ì… (ìˆœì„œëŒ€ë¡œ)
      const tables = ['projects', 'pages', 'elements', 'design_themes', 'design_tokens'];

      for (const table of tables) {
        if (data[table] && data[table].length > 0) {
          await tx.insert(table, data[table]);
        }
      }
    });

    console.log(`âœ… Imported from ${filePath}`);
  }

  /**
   * Export to SQL dump
   */
  async exportToSql(filePath: string): Promise<void> {
    // pg_dump ìŠ¤íƒ€ì¼ SQL ìƒì„±
    const tables = ['projects', 'pages', 'elements', 'design_themes', 'design_tokens'];
    let sql = '';

    for (const table of tables) {
      const rows = await db.select(table);

      if (rows.length > 0) {
        sql += `-- Table: ${table}\n`;
        sql += `DELETE FROM ${table};\n`;

        for (const row of rows) {
          const keys = Object.keys(row);
          const values = keys.map(key => {
            const value = row[key];
            if (value === null) return 'NULL';
            if (typeof value === 'object') return `'${JSON.stringify(value)}'::jsonb`;
            if (typeof value === 'string') return `'${value.replace(/'/g, "''")}'`;
            return value;
          });

          sql += `INSERT INTO ${table} (${keys.join(', ')}) VALUES (${values.join(', ')});\n`;
        }

        sql += '\n';
      }
    }

    fs.writeFileSync(filePath, sql);
    console.log(`âœ… Exported SQL to ${filePath}`);
  }
}

export const exportImport = new ExportImportService();
```

---

### 2.4. ë””ìŠ¤í¬ ê³µê°„ ê´€ë¦¬

#### ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ëª¨ë‹ˆí„°ë§

```typescript
// src/services/database/monitoring.ts

import { db } from './index';

export class DatabaseMonitoring {
  /**
   * Get database size statistics
   */
  async getSizeStats(): Promise<{
    totalSize: number;
    tablesSizes: Array<{ table: string; size: number }>;
    indexesSize: number;
  }> {
    const totalSize = await db.getDbSize();

    // í…Œì´ë¸”ë³„ í¬ê¸°
    const tables = ['projects', 'pages', 'elements', 'design_themes', 'design_tokens'];
    const tablesSizes: Array<{ table: string; size: number }> = [];

    for (const table of tables) {
      const result = await db.query(`
        SELECT pg_total_relation_size('${table}') as size
      `);
      tablesSizes.push({ table, size: parseInt(result[0].size) });
    }

    // ì¸ë±ìŠ¤ í¬ê¸°
    const indexResult = await db.query(`
      SELECT SUM(pg_indexes_size(tablename::regclass)) as size
      FROM pg_tables
      WHERE schemaname = 'public'
    `);
    const indexesSize = parseInt(indexResult[0].size || '0');

    return {
      totalSize,
      tablesSizes,
      indexesSize,
    };
  }

  /**
   * Vacuum and analyze database
   */
  async optimize(): Promise<void> {
    await db.vacuum();
    await db.query('ANALYZE');
    console.log('âœ… Database optimized');
  }

  /**
   * Check if cleanup is needed
   */
  async needsCleanup(maxSizeMb: number = 500): Promise<boolean> {
    const stats = await this.getSizeStats();
    const sizeMb = stats.totalSize / 1024 / 1024;

    return sizeMb > maxSizeMb;
  }
}

export const dbMonitoring = new DatabaseMonitoring();
```

---

## 3ï¸âƒ£ ë™ê¸°í™” í”„ë¡œí† íƒ€ì…

### 3.1. ì¦ë¶„ ë™ê¸°í™” ì„¤ê³„

#### ë³€ê²½ ì¶”ì  ì‹œìŠ¤í…œ

```typescript
// src/services/sync/changeTracking.ts

/**
 * Change Log Table Schema
 */
export interface ChangeLog {
  id: string;
  table_name: string;
  record_id: string;
  operation: 'INSERT' | 'UPDATE' | 'DELETE';
  data: any;
  synced: boolean;
  created_at: Date;
}

/**
 * Setup change tracking triggers
 */
export async function setupChangeTracking(db: any): Promise<void> {
  // ë³€ê²½ ë¡œê·¸ í…Œì´ë¸” ìƒì„±
  await db.query(`
    CREATE TABLE IF NOT EXISTS _change_log (
      id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
      table_name TEXT NOT NULL,
      record_id UUID NOT NULL,
      operation TEXT NOT NULL CHECK (operation IN ('INSERT', 'UPDATE', 'DELETE')),
      data JSONB,
      synced BOOLEAN DEFAULT FALSE,
      created_at TIMESTAMPTZ DEFAULT NOW()
    );

    CREATE INDEX IF NOT EXISTS idx_change_log_synced ON _change_log(synced);
    CREATE INDEX IF NOT EXISTS idx_change_log_table ON _change_log(table_name);
  `);

  // íŠ¸ë¦¬ê±° í•¨ìˆ˜ ìƒì„±
  await db.query(`
    CREATE OR REPLACE FUNCTION log_change()
    RETURNS TRIGGER AS $$
    BEGIN
      IF (TG_OP = 'DELETE') THEN
        INSERT INTO _change_log (table_name, record_id, operation, data)
        VALUES (TG_TABLE_NAME, OLD.id, 'DELETE', row_to_json(OLD));
        RETURN OLD;
      ELSIF (TG_OP = 'UPDATE') THEN
        INSERT INTO _change_log (table_name, record_id, operation, data)
        VALUES (TG_TABLE_NAME, NEW.id, 'UPDATE', row_to_json(NEW));
        RETURN NEW;
      ELSIF (TG_OP = 'INSERT') THEN
        INSERT INTO _change_log (table_name, record_id, operation, data)
        VALUES (TG_TABLE_NAME, NEW.id, 'INSERT', row_to_json(NEW));
        RETURN NEW;
      END IF;
    END;
    $$ LANGUAGE plpgsql;
  `);

  // ê° í…Œì´ë¸”ì— íŠ¸ë¦¬ê±° ì¶”ê°€
  const tables = ['projects', 'pages', 'elements', 'design_themes', 'design_tokens'];

  for (const table of tables) {
    await db.query(`
      DROP TRIGGER IF EXISTS ${table}_change_log ON ${table};
      CREATE TRIGGER ${table}_change_log
        AFTER INSERT OR UPDATE OR DELETE ON ${table}
        FOR EACH ROW EXECUTE FUNCTION log_change();
    `);
  }

  console.log('âœ… Change tracking setup complete');
}
```

#### ë™ê¸°í™” ì„œë¹„ìŠ¤

```typescript
// src/services/sync/syncService.ts

import { db as localDb } from '../database';
import { supabase } from '../database/supabaseAdapter';

export class SyncService {
  private isSyncing = false;
  private syncInterval: NodeJS.Timeout | null = null;

  /**
   * Start automatic sync
   */
  start(intervalMinutes: number = 5) {
    if (this.syncInterval) {
      this.stop();
    }

    // ì¦‰ì‹œ ë™ê¸°í™” ì‹¤í–‰
    this.sync();

    // ì£¼ê¸°ì  ë™ê¸°í™”
    this.syncInterval = setInterval(() => {
      this.sync();
    }, intervalMinutes * 60 * 1000);

    console.log(`âœ… Auto sync started (interval: ${intervalMinutes}min)`);
  }

  /**
   * Stop automatic sync
   */
  stop() {
    if (this.syncInterval) {
      clearInterval(this.syncInterval);
      this.syncInterval = null;
      console.log('âœ… Auto sync stopped');
    }
  }

  /**
   * Manual sync
   */
  async sync(): Promise<{ pushed: number; pulled: number; conflicts: number }> {
    if (this.isSyncing) {
      console.log('âš ï¸ Sync already in progress');
      return { pushed: 0, pulled: 0, conflicts: 0 };
    }

    this.isSyncing = true;

    try {
      console.log('ğŸ”„ Starting sync...');

      // 1. Push local changes to Supabase
      const pushed = await this.pushChanges();

      // 2. Pull remote changes from Supabase
      const pulled = await this.pullChanges();

      // 3. Resolve conflicts
      const conflicts = await this.resolveConflicts();

      console.log(`âœ… Sync complete: Pushed ${pushed}, Pulled ${pulled}, Conflicts ${conflicts}`);

      return { pushed, pulled, conflicts };
    } catch (error) {
      console.error('âŒ Sync failed:', error);
      throw error;
    } finally {
      this.isSyncing = false;
    }
  }

  /**
   * Push local changes to Supabase
   */
  private async pushChanges(): Promise<number> {
    // ë™ê¸°í™”ë˜ì§€ ì•Šì€ ë³€ê²½ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
    const changes = await localDb.query<ChangeLog>(
      'SELECT * FROM _change_log WHERE synced = FALSE ORDER BY created_at ASC'
    );

    let pushedCount = 0;

    for (const change of changes) {
      try {
        switch (change.operation) {
          case 'INSERT':
            await supabase.insert(change.table_name, change.data);
            break;

          case 'UPDATE':
            await supabase.update(change.table_name, change.record_id, change.data);
            break;

          case 'DELETE':
            await supabase.delete(change.table_name, change.record_id);
            break;
        }

        // ë™ê¸°í™” ì™„ë£Œ í‘œì‹œ
        await localDb.query(
          'UPDATE _change_log SET synced = TRUE WHERE id = $1',
          [change.id]
        );

        pushedCount++;
      } catch (error) {
        console.error(`âŒ Failed to push change ${change.id}:`, error);
        // ê³„ì† ì§„í–‰ (ë‹¤ìŒ ë™ê¸°í™” ë•Œ ì¬ì‹œë„)
      }
    }

    return pushedCount;
  }

  /**
   * Pull remote changes from Supabase
   */
  private async pullChanges(): Promise<number> {
    // ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì´í›„ ë³€ê²½ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
    const lastSync = await this.getLastSyncTime();
    const tables = ['projects', 'pages', 'elements', 'design_themes', 'design_tokens'];

    let pulledCount = 0;

    for (const table of tables) {
      const remoteData = await supabase.select(table, {
        // updated_at > lastSync
      });

      for (const record of remoteData) {
        // ë¡œì»¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        const localRecord = await localDb.select(table, {
          where: { id: record.id },
        });

        if (localRecord.length === 0) {
          // ìƒˆ ë ˆì½”ë“œ ì‚½ì…
          await localDb.insert(table, record);
          pulledCount++;
        } else if (new Date(record.updated_at) > new Date(localRecord[0].updated_at)) {
          // ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ ê°±ì‹ 
          await localDb.update(table, record.id, record);
          pulledCount++;
        }
      }
    }

    // ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸
    await this.updateLastSyncTime();

    return pulledCount;
  }

  /**
   * Resolve conflicts
   */
  private async resolveConflicts(): Promise<number> {
    // TODO: ì¶©ëŒ í•´ê²° ë¡œì§ êµ¬í˜„
    // 1. ë™ì¼í•œ ë ˆì½”ë“œê°€ ë¡œì»¬/ì›ê²© ëª¨ë‘ì—ì„œ ìˆ˜ì •ëœ ê²½ìš° ê°ì§€
    // 2. ì¶©ëŒ í•´ê²° ì •ì±… ì ìš© (íƒ€ì„ìŠ¤íƒ¬í”„ ìš°ì„ , ì‚¬ìš©ì ì„ íƒ ë“±)
    return 0;
  }

  /**
   * Get last sync time
   */
  private async getLastSyncTime(): Promise<Date> {
    const result = await localDb.query(`
      SELECT value FROM _sync_metadata WHERE key = 'last_sync_time'
    `);

    if (result.length === 0) {
      return new Date(0); // 1970-01-01 (ìµœì´ˆ ë™ê¸°í™”)
    }

    return new Date(result[0].value);
  }

  /**
   * Update last sync time
   */
  private async updateLastSyncTime(): Promise<void> {
    await localDb.query(`
      INSERT INTO _sync_metadata (key, value)
      VALUES ('last_sync_time', $1)
      ON CONFLICT (key) DO UPDATE SET value = $1
    `, [new Date().toISOString()]);
  }
}

export const syncService = new SyncService();
```

---

### 3.2. ì¶©ëŒ í•´ê²° ì •ì±…

#### ì¶©ëŒ ìœ í˜• ë° í•´ê²° ì „ëµ

```typescript
// src/services/sync/conflictResolution.ts

export type ConflictResolutionStrategy =
  | 'local-wins'      // ë¡œì»¬ ìš°ì„ 
  | 'remote-wins'     // ì›ê²© ìš°ì„ 
  | 'timestamp-wins'  // ìµœì‹  íƒ€ì„ìŠ¤íƒ¬í”„ ìš°ì„ 
  | 'manual';         // ì‚¬ìš©ì ì„ íƒ

export interface Conflict {
  table: string;
  recordId: string;
  localData: any;
  remoteData: any;
  localUpdatedAt: Date;
  remoteUpdatedAt: Date;
}

export class ConflictResolver {
  constructor(private strategy: ConflictResolutionStrategy = 'timestamp-wins') {}

  /**
   * Resolve conflict
   */
  async resolve(conflict: Conflict): Promise<any> {
    switch (this.strategy) {
      case 'local-wins':
        return conflict.localData;

      case 'remote-wins':
        return conflict.remoteData;

      case 'timestamp-wins':
        return conflict.localUpdatedAt > conflict.remoteUpdatedAt
          ? conflict.localData
          : conflict.remoteData;

      case 'manual':
        return await this.manualResolve(conflict);

      default:
        throw new Error(`Unknown strategy: ${this.strategy}`);
    }
  }

  /**
   * Manual conflict resolution (UI required)
   */
  private async manualResolve(conflict: Conflict): Promise<any> {
    // Electronì—ì„œ IPCë¡œ UIì— ì¶©ëŒ ì•Œë¦¼
    // ì‚¬ìš©ìê°€ ì„ íƒí•  ë•Œê¹Œì§€ ëŒ€ê¸°
    // TODO: UI êµ¬í˜„ í•„ìš”

    return conflict.localData; // ì„ì‹œ: ë¡œì»¬ ìš°ì„ 
  }
}
```

---

### 3.3. ì˜¤í”„ë¼ì¸ íì‰

#### ì˜¤í”„ë¼ì¸ ì‘ì—… í

```typescript
// src/services/sync/offlineQueue.ts

export interface QueuedOperation {
  id: string;
  type: 'CREATE' | 'UPDATE' | 'DELETE';
  table: string;
  data: any;
  createdAt: Date;
  retryCount: number;
}

export class OfflineQueue {
  private queue: QueuedOperation[] = [];

  /**
   * Add operation to queue
   */
  add(operation: Omit<QueuedOperation, 'id' | 'createdAt' | 'retryCount'>) {
    this.queue.push({
      ...operation,
      id: crypto.randomUUID(),
      createdAt: new Date(),
      retryCount: 0,
    });

    console.log(`â• Added to offline queue: ${operation.type} ${operation.table}`);
  }

  /**
   * Process queue (when online)
   */
  async process(): Promise<{ success: number; failed: number }> {
    let success = 0;
    let failed = 0;

    for (const operation of this.queue) {
      try {
        // Supabaseì— ì—…ë¡œë“œ
        await this.uploadOperation(operation);

        // íì—ì„œ ì œê±°
        this.queue = this.queue.filter(op => op.id !== operation.id);
        success++;
      } catch (error) {
        console.error(`âŒ Failed to process ${operation.id}:`, error);

        operation.retryCount++;

        if (operation.retryCount >= 3) {
          // 3íšŒ ì‹¤íŒ¨ ì‹œ íì—ì„œ ì œê±°
          this.queue = this.queue.filter(op => op.id !== operation.id);
          failed++;
        }
      }
    }

    console.log(`âœ… Processed queue: ${success} success, ${failed} failed`);

    return { success, failed };
  }

  /**
   * Upload operation to Supabase
   */
  private async uploadOperation(operation: QueuedOperation): Promise<void> {
    const { type, table, data } = operation;

    switch (type) {
      case 'CREATE':
        await supabase.insert(table, data);
        break;

      case 'UPDATE':
        await supabase.update(table, data.id, data);
        break;

      case 'DELETE':
        await supabase.delete(table, data.id);
        break;
    }
  }

  /**
   * Get queue size
   */
  size(): number {
    return this.queue.length;
  }

  /**
   * Clear queue
   */
  clear() {
    this.queue = [];
  }
}

export const offlineQueue = new OfflineQueue();
```

---

## 4ï¸âƒ£ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸

### 4.1. í¬ë˜ì‹œ ë³µêµ¬ í…ŒìŠ¤íŠ¸

```typescript
// tests/stability/crash-recovery.test.ts

describe('Crash Recovery Tests', () => {
  it('should recover from sudden shutdown during write', async () => {
    // íŠ¸ëœì­ì…˜ ì¤‘ê°„ì— ê°•ì œ ì¢…ë£Œ ì‹œë®¬ë ˆì´ì…˜
    // ...
  });

  it('should maintain data integrity after power failure', async () => {
    // ì „ì› ì°¨ë‹¨ ì‹œë®¬ë ˆì´ì…˜
    // ...
  });
});
```

### 4.2. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦

```typescript
// tests/stability/data-integrity.test.ts

describe('Data Integrity Tests', () => {
  it('should maintain foreign key constraints', async () => {
    // ì™¸ë˜í‚¤ ì œì•½ ì¡°ê±´ í…ŒìŠ¤íŠ¸
    // ...
  });

  it('should enforce unique constraints', async () => {
    // ê³ ìœ  ì œì•½ ì¡°ê±´ í…ŒìŠ¤íŠ¸
    // ...
  });
});
```

---

## âœ… ê²€ì¦ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

ë°°í¬ ì „ ëª¨ë“  í•­ëª© ì™„ë£Œ í™•ì¸:

- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì™„ë£Œ (CRUD, RPC, ë™ì‹œì„±, ëŒ€ìš©ëŸ‰)
- [ ] ì €ì¥ ê²½ë¡œ ì •ì±… ìˆ˜ë¦½
- [ ] ìë™ ë°±ì—… ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] Export/Import ê¸°ëŠ¥ êµ¬í˜„
- [ ] ë””ìŠ¤í¬ ê³µê°„ ëª¨ë‹ˆí„°ë§ êµ¬í˜„
- [ ] ë³€ê²½ ì¶”ì  ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë™ê¸°í™” ì„œë¹„ìŠ¤ êµ¬í˜„
- [ ] ì¶©ëŒ í•´ê²° ì •ì±… ìˆ˜ë¦½
- [ ] ì˜¤í”„ë¼ì¸ íì‰ êµ¬í˜„
- [ ] í¬ë˜ì‹œ ë³µêµ¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ë¬¸ì„œ ì‘ì„±

---

## ğŸ“š ì¶”ê°€ ì°¸ê³  ìë£Œ

- [PGlite ê³µì‹ ë¬¸ì„œ](https://github.com/electric-sql/pglite)
- [Electron ë°ì´í„° ì €ì¥ Best Practices](https://www.electronjs.org/docs/latest/tutorial/using-native-node-modules)
- [Offline-First ì„¤ê³„ íŒ¨í„´](https://offlinefirst.org/)

---

**ì‘ì„±ì**: Claude Code
**ì‘ì„±ì¼**: 2025-11-07
**ë²„ì „**: 1.0.0
